import urllib.request
import json
import re
import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from pymongo import MongoClient
import pprint
from bs4 import BeautifulSoup

# --- âš™ï¸ ì„¤ì • ì •ë³´ ---
MONGO_CONFIG = {
    'host': '192.168.0.222',
    'port': 27017,
    'username': 'kevin',
    'password': 'pass123#',
    'db_name': 'jongro'
}
RESTAURANTS_COLLECTION = 'RESTAURANTS_GENERAL'
CRAWLED_COLLECTION = 'crawled_nave_blogs'
NAVER_CLIENT_ID = "46_7kjfK4xilqSfTXXK8"
NAVER_CLIENT_SECRET = "ZmiT61_9du"
BLOGS_PER_RESTAURANT = 10

def get_dong_top5_from_mongodb(db):
    print("--- 1ë‹¨ê³„: TOP 5 ë§›ì§‘ ì¶”ì¶œ ì‹œì‘ ---")
    collection = db[RESTAURANTS_COLLECTION]
    pipeline = [
        {'$match': {'admin_dong': {'$exists': True, '$ne': 'ë¶„ë¥˜ë¶ˆê°€'}}},
        {'$sort': {'weighted_score': -1}},
        {'$group': {
            '_id': '$admin_dong',
            # ğŸ”¥ ë³€ê²½ì  1: nameê³¼ í•¨ê»˜ categoryë„ ê°€ì ¸ì˜¤ë„ë¡ $push ìˆ˜ì •
            'restaurants': {'$push': {'name': '$name', 'category': '$category'}}
        }},
        {'$project': {'dong': '$_id', 'top5_restaurants': {'$slice': ['$restaurants', 5]}, '_id': 0}}
    ]
    target_list = list(collection.aggregate(pipeline))
    print(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: ì´ {len(target_list)}ê°œ ë™ì˜ ë§›ì§‘ ì •ë³´ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
    return target_list

def crawl_and_save_blogs_incrementally(dong_top5_list, db):
    print("\n--- 2ë‹¨ê³„: Naver API + Selenium í¬ë¡¤ë§ (ì¦‰ì‹œ ì €ì¥ ë°©ì‹) ì‹œì‘ ---")
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        print("âŒ ì˜¤ë¥˜: Naver API Keyë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        return

    crawled_collection = db[CRAWLED_COLLECTION]
    crawled_collection.delete_many({})
    print(f"âœ… '{CRAWLED_COLLECTION}' ì»¬ë ‰ì…˜ì„ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")
    
    total_saved_count = 0
    
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--log-level=3")
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)

    try:
        for dong_data in dong_top5_list:
            dong_name = dong_data['dong']
            for restaurant in dong_data['top5_restaurants']:
                restaurant_name = restaurant['name']
                # ğŸ”¥ ë³€ê²½ì  2: restaurant ë”•ì…”ë„ˆë¦¬ì—ì„œ category ì •ë³´ë„ ê°€ì ¸ì˜´
                restaurant_category = restaurant['category']
                
                query = f"{dong_name} {restaurant_name}"
                encText = urllib.parse.quote(query)
                print(f"\nğŸ” '{query}' ê²€ìƒ‰ ì‹œì‘...")
                
                # ... (API í˜¸ì¶œ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼) ...
                api_url = f"https://openapi.naver.com/v1/search/blog?query={encText}&display={BLOGS_PER_RESTAURANT}&sort=sim"
                request = urllib.request.Request(api_url)
                request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
                request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
                blog_post_info = []
                try:
                    response = urllib.request.urlopen(request)
                    rescode = response.getcode()
                    if rescode == 200:
                        response_body = response.read()
                        data = json.loads(response_body.decode('utf-8'))['items']
                        for item in data:
                            if 'blog.naver' in item['link']:
                                title = re.sub('<[^>]*>', '', item['title'])
                                blog_post_info.append({'link': item['link'], 'title': title, 'postdate': item['postdate']})
                        print(f"  âœ… API í˜¸ì¶œ ì„±ê³µ: {len(blog_post_info)}ê°œì˜ ë¸”ë¡œê·¸ ëª©ë¡ í™•ë³´.")
                    else:
                        print(f"  âŒ API ì˜¤ë¥˜ ë°œìƒ: Error Code {rescode}")
                        continue
                except Exception as e:
                    print(f"  â—ï¸ API ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

                for i, post in enumerate(blog_post_info):
                    print(f"  - [{i+1}/{len(blog_post_info)}] ë¸”ë¡œê·¸ ë°©ë¬¸ ë° ìˆ˜ì§‘ ì‹œì‘...")
                    time.sleep(random.uniform(1.2, 1.8))
                    try:
                        driver.get(post['link'])
                        if driver.find_elements(By.ID, "mainFrame"):
                            driver.switch_to.frame(driver.find_element(By.ID, "mainFrame"))
                        
                        source = driver.page_source
                        html = BeautifulSoup(source, "lxml")
                        content = None
                        if html.select_one("div.se-main-container"):
                            content = html.select_one("div.se-main-container").get_text(separator='\n', strip=True)
                        elif html.select_one("div#postViewArea"):
                            content = html.select_one("div#postViewArea").get_text(separator='\n', strip=True)
                        
                        if content:
                            document = {
                                'admin_dong': dong_name, 
                                'restaurant_name': restaurant_name,
                                'category': restaurant_category, # ğŸ”¥ ë³€ê²½ì  3: ì €ì¥í•  documentì— category ì¶”ê°€
                                'title': post['title'], 
                                'post_date': post['postdate'],
                                'blog_url': post['link'],
                                'blog_content': content
                            }
                            crawled_collection.insert_one(document)
                            total_saved_count += 1
                            print(f"    ğŸ’¾ ì €ì¥ ì„±ê³µ! (ì´ {total_saved_count}ê°œ)")
                        else:
                            print("    âš ï¸ ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    except Exception as e:
                        print(f"  â—ï¸ ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {post['link']} - {e}")
                    finally:
                        driver.switch_to.default_content()
    finally:
        driver.quit()
        print("\n--- Selenium ë“œë¼ì´ë²„ ì¢…ë£Œ ---")

# --- ë©”ì¸ ì½”ë“œ ì‹¤í–‰ ---
if __name__ == "__main__":
    client = None
    try:
        uri = f"mongodb://{MONGO_CONFIG['username']}:{MONGO_CONFIG['password']}@{MONGO_CONFIG['host']}:{MONGO_CONFIG['port']}/"
        client = MongoClient(uri, serverSelectionTimeoutMS=5000)
        db = client[MONGO_CONFIG['db_name']]
        target_list = get_dong_top5_from_mongodb(db)
        if target_list:
            crawl_and_save_blogs_incrementally(target_list, db)
    except Exception as e:
        print(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if client:
            client.close()
        print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")