import urllib.request
import json
import re
import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from pymongo import MongoClient
import pprint
from bs4 import BeautifulSoup
import pandas as pd

MONGO_CONFIG = {
    'host': '192.168.0.222',
    'port': 27017,
    'username': 'kevin',
    'password': 'pass123#',
    'db_name': 'jongro'
}
RESTAURANTS_COLLECTION = 'RESTAURANTS_GENERAL'
CRAWLED_COLLECTION = 'naverblogs_cleansing'
NAVER_CLIENT_ID = "46_7kjfK4xilqSfTXXK8"
NAVER_CLIENT_SECRET = "ZmiT61_9du"
BLOGS_PER_RESTAURANT = 100

def get_dong_top5_from_mongodb(db):
    print("--- 1ë‹¨ê³„: ë§›ì§‘ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ê°€ì¤‘ ì ìˆ˜ ê³„ì‚° ì‹œì‘ ---")
    collection = db[RESTAURANTS_COLLECTION]
    
    # 1. MongoDBì—ì„œ í•„ìš”í•œ ë°ì´í„° ì „ì²´ë¥¼ ë¶ˆëŸ¬ì™€ DataFrameìœ¼ë¡œ ë³€í™˜
    #    (ì£¼ì˜: ratingê³¼ visitor_reviews í•„ë“œê°€ DBì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤)
    try:
        data = list(collection.find(
            {'admin_dong': {'$exists': True, '$ne': 'ë¶„ë¥˜ë¶ˆê°€'}},
            {'name': 1, 'category': 1, 'admin_dong': 1, 'rating': 1, 'visitor_reviews': 1, '_id': 0}
        ))
        if not data:
            print("âŒ ì˜¤ë¥˜: RESTAURANTS_GENERAL ì»¬ë ‰ì…˜ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        df = pd.DataFrame(data)
        
        # 2. ë°ì´í„° í´ë¦¬ë‹ (ìˆ«ìê°€ ì•„ë‹Œ ê°’ì´ë‚˜ ë¹ˆ ê°’ì„ 0ìœ¼ë¡œ ì²˜ë¦¬)
        df['rating'] = pd.to_numeric(df['rating'], errors='coerce').fillna(0)
        df['visitor_reviews'] = pd.to_numeric(df['visitor_reviews'], errors='coerce').fillna(0)
        
    except Exception as e:
        print(f"âŒ DBì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

    # 3. ê°€ì¤‘ í‰ì (weighted_score) ê³„ì‚°
    # C = ì „ì²´ ì‹ë‹¹ì˜ í‰ê·  í‰ì 
    C = df['rating'].mean()
    # m = í‰ì ì˜ ì‹ ë¢°ë„ë¥¼ ê²°ì •í•˜ëŠ” ìµœì†Œ ë¦¬ë·° ìˆ˜ (ìƒìˆ˜ê°’)
    m = 200 

    def calculate_weighted_score(row):
        v = row['visitor_reviews']
        R = row['rating']
        # ë² ì´ì¦ˆ í‰ê·  ê³µì‹ ì ìš©
        return (v / (v + m)) * R + (m / (v + m)) * C

    df['weighted_score'] = df.apply(calculate_weighted_score, axis=1)

    # 4. ê³„ì‚°ëœ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê° ë™(dong)ë³„ Top 5 ì„ ì •
    top15_df = df.sort_values('weighted_score', ascending=False).groupby('admin_dong').head(15)

    # 5. ë‹¤ìŒ ë‹¨ê³„(í¬ë¡¤ë§)ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë°ì´í„° í¬ë§· ë³€ê²½
    target_list = []
    for dong, group in top15_df.groupby('admin_dong'):
        dong_data = {
            'dong': dong,
            'top5_restaurants': group[['name', 'category']].to_dict('records')
        }
        target_list.append(dong_data)
        
    print(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: ê°€ì¤‘ ì ìˆ˜ ê³„ì‚° í›„ {len(target_list)}ê°œ ë™ì˜ Top 5 ë§›ì§‘ ì •ë³´ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
    return target_list

def crawl_and_save_blogs_incrementally(dong_top5_list, db):
    print("\n--- 2ë‹¨ê³„: Naver API + Selenium í¬ë¡¤ë§ (ì¦‰ì‹œ ì €ì¥ ë°©ì‹) ì‹œì‘ ---")
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        print("âŒ ì˜¤ë¥˜: Naver API Keyë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        return

    crawled_collection = db[CRAWLED_COLLECTION]
    #crawled_collection.delete_many({})
    print(f"âœ… '{CRAWLED_COLLECTION}' ì»¬ë ‰ì…˜ì„ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")
    
    total_saved_count = 0
    
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--log-level=3")
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)

    try:
        for dong_data in dong_top5_list:
            dong_name = dong_data['dong']
            for restaurant in dong_data['top5_restaurants']:
                restaurant_name = restaurant['name']
                # ğŸ”¥ ë³€ê²½ì  2: restaurant ë”•ì…”ë„ˆë¦¬ì—ì„œ category ì •ë³´ë„ ê°€ì ¸ì˜´
                restaurant_category = restaurant['category']
                
                query = f"{dong_name} {restaurant_name}"
                encText = urllib.parse.quote(query)
                print(f"\nğŸ” '{query}' ê²€ìƒ‰ ì‹œì‘...")
                
                # ... (API í˜¸ì¶œ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼) ...
                api_url = f"https://openapi.naver.com/v1/search/blog?query={encText}&display={BLOGS_PER_RESTAURANT}&sort=sim"
                request = urllib.request.Request(api_url)
                request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
                request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
                blog_post_info = []
                try:
                    response = urllib.request.urlopen(request)
                    rescode = response.getcode()
                    if rescode == 200:
                        response_body = response.read()
                        data = json.loads(response_body.decode('utf-8'))['items']
                        for item in data:
                            if 'blog.naver' in item['link']:
                                title = re.sub('<[^>]*>', '', item['title'])
                                blog_post_info.append({'link': item['link'], 'title': title, 'postdate': item['postdate']})
                        print(f"  âœ… API í˜¸ì¶œ ì„±ê³µ: {len(blog_post_info)}ê°œì˜ ë¸”ë¡œê·¸ ëª©ë¡ í™•ë³´.")
                    else:
                        print(f"  âŒ API ì˜¤ë¥˜ ë°œìƒ: Error Code {rescode}")
                        continue
                except Exception as e:
                    print(f"  â—ï¸ API ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

                for i, post in enumerate(blog_post_info):
                    print(f"  - [{i+1}/{len(blog_post_info)}] ë¸”ë¡œê·¸ ë°©ë¬¸ ë° ìˆ˜ì§‘ ì‹œì‘...")
                    time.sleep(random.uniform(1.2, 1.8))
                    try:
                        driver.get(post['link'])
                        if driver.find_elements(By.ID, "mainFrame"):
                            driver.switch_to.frame(driver.find_element(By.ID, "mainFrame"))
                        
                        source = driver.page_source
                        html = BeautifulSoup(source, "lxml")
                        content = None
                        if html.select_one("div.se-main-container"):
                            content = html.select_one("div.se-main-container").get_text(separator='\n', strip=True)
                        elif html.select_one("div#postViewArea"):
                            content = html.select_one("div#postViewArea").get_text(separator='\n', strip=True)
                        
                        if content:
                            document = {
                                'admin_dong': dong_name, 
                                'restaurant_name': restaurant_name,
                                'category': restaurant_category, 
                                'title': post['title'], 
                                'post_date': post['postdate'],
                                'blog_url': post['link'],
                                'blog_content': content
                            }
                            crawled_collection.insert_one(document)
                            total_saved_count += 1
                            print(f"    ğŸ’¾ ì €ì¥ ì„±ê³µ! (ì´ {total_saved_count}ê°œ)")
                        else:
                            print("    âš ï¸ ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    except Exception as e:
                        print(f"  â—ï¸ ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {post['link']} - {e}")
                    finally:
                        driver.switch_to.default_content()
    finally:
        driver.quit()
        print("\n--- Selenium ë“œë¼ì´ë²„ ì¢…ë£Œ ---")

if __name__ == "__main__":
    client = None
    try:
        uri = f"mongodb://{MONGO_CONFIG['username']}:{MONGO_CONFIG['password']}@{MONGO_CONFIG['host']}:{MONGO_CONFIG['port']}/"
        client = MongoClient(uri, serverSelectionTimeoutMS=5000)
        db = client[MONGO_CONFIG['db_name']]
        target_list = get_dong_top5_from_mongodb(db)
        if target_list:
            crawl_and_save_blogs_incrementally(target_list, db)
    except Exception as e:
        print(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if client:
            client.close()
        print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")